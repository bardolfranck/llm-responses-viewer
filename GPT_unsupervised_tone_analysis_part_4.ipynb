{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "Xr8UA0su3xdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlYdRkYuuUWR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n"
      ],
      "metadata": {
        "id": "o0uJll2cM5gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objective:\n",
        "Show that question tone (positive, neutral, negative) induces measurable differences in LLM-generated response embeddings.\n",
        "If the \"emotional rebound\" is real: embeddings of responses to negative questions should move closer (in vector space) to embeddings of responses to neutral or positive questions (rather than forming an isolated negative cluster).\n"
      ],
      "metadata": {
        "id": "nP01ddLwNFKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words: the \"emotional rebound\" or tone correction by the AI leaves a trace in the space of representations.\n",
        "\n",
        "How to proceed:\n",
        "For each question, you have a triplet: the same question asked with three different tones.\n",
        "\n",
        "For each answer produced by the LLM, you retrieve the embedding via the API.\n",
        "\n",
        "Clustering analysis :\n",
        "\n",
        "You can visualize (PCA/t-SNE/UMAP) all the embeddings of the responses, by coloring the points according to the tone of the prompt (pos/neutral/neg).\n",
        "\n",
        "Alternatively, you can analyze the distance between embeddings:\n",
        "\n",
        "Is the answer to the negative question systematically \"closer\" to the neutral/positive answer?\n",
        "\n",
        "Do negative-toned answers move away from one region of space, or on the contrary, are they \"recovered\" towards the same cluster?\n",
        "\n",
        "Clustering (k-means, DBSCAN, etc.) can also reveal groupings according to tone, or on the contrary a normalization effect.\n",
        "\n",
        "Your point:\n",
        "If \"emotional rebound\" is real: embeddings of responses to negative questions should move closer (in vector space) to embeddings of responses to neutral or positive questions (rather than forming an isolated negative cluster).\n",
        "\n",
        "If the LLM corrects the negative tone: you'll observe few \"negative clusters\"; on the contrary, the majority of embeddings will fall into the same \"neutral/positive\" space.\n",
        "\n",
        "You can quantify: is the distance between \"negative tone response\" and \"neutral/positive response\" embeddings significantly small?\n",
        "\n",
        "Ultimate goal: to show that the emotional smoothing effect operates not only in the text, but in the latent space of the model.\n",
        "\n",
        "In short: you want to prove that tone neutralization by LLM is objectivable and measurable in the embedding space, confirming the robustness and systematicity of the bias."
      ],
      "metadata": {
        "id": "2braui--Mbjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VERBOSE = True"
      ],
      "metadata": {
        "id": "yX6QxOd6eg0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import openai\n",
        "\n",
        "# Saisie manuelle et masquée\n",
        "if True:\n",
        "  openai.api_key = getpass.getpass(\"Entre ta clé API OpenAI : \")\n",
        "else:\n",
        "  openai.api_key = os.getenv(\"OPENAI_API_KEY\")  #  définie dans l’environnement"
      ],
      "metadata": {
        "id": "JwOKlfz-6WkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#openai.api_key = \"sk-...\"  # Remplace par ta clé\n",
        "client = openai.OpenAI(api_key=openai.api_key)"
      ],
      "metadata": {
        "id": "LYyTrzwg2Ah9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(client.models.list())"
      ],
      "metadata": {
        "id": "0ElsdmSLRkQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VERBOSE = True"
      ],
      "metadata": {
        "id": "M5rzNSgWK4jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data"
      ],
      "metadata": {
        "id": "6RSk9_1XQfBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/output-LLM-responses-v3.csv')"
      ],
      "metadata": {
        "id": "vobeeRddK4O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "FkO1BaW4MLXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "euLwBXtwcv9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cond_controversal = (df['domaine'] == 'opinion')|(df['domaine'] == 'science')| (df['domaine'] == 'société') | (df['domaine'] == 'immigration') | (df['domaine'] == 'climat')\n",
        "df_controversal = df[cond_controversal]"
      ],
      "metadata": {
        "id": "0cbLHqdYbtdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Get embeddings"
      ],
      "metadata": {
        "id": "rdksXr5KQ3Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []"
      ],
      "metadata": {
        "id": "A4B3hdUMQ9dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpt = 0\n",
        "MAX = np.inf#3*25\n",
        "for _, row in tqdm(df.iterrows() ,  total=len(df)):\n",
        "    texte = row['reponse']\n",
        "    response = openai.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=texte\n",
        "    )\n",
        "    embedding = np.array(response.data[0].embedding)\n",
        "    embeddings.append(embedding)\n",
        "    if False : time.sleep(1)\n",
        "    cpt = cpt+1\n",
        "    if cpt > MAX:\n",
        "      break\n",
        "\n"
      ],
      "metadata": {
        "id": "y_1wG5_oQr9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cpt"
      ],
      "metadata": {
        "id": "gA1X1iKpUANj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f'type :{type(embedding)} ; len : {len(embedding)}')"
      ],
      "metadata": {
        "id": "EyUBZM_NRyDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings)"
      ],
      "metadata": {
        "id": "xEAaAcpRR6Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster embeddings"
      ],
      "metadata": {
        "id": "Cv6vPWgpWhEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label encoder"
      ],
      "metadata": {
        "id": "QHTIFfznXnmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(df['label'])\n",
        "labels = le.transform(df['label'])\n",
        "labels.shape"
      ],
      "metadata": {
        "id": "4e1TUSUSXdle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_0 = np.where(labels == 0)\n",
        "idx_1 = np.where(labels == 1)\n",
        "idx_2 = np.where(labels == 2)"
      ],
      "metadata": {
        "id": "KHIXG1NgZm4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le.inverse_transform([0,1,2])"
      ],
      "metadata": {
        "id": "aXgMuvG-PneF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA"
      ],
      "metadata": {
        "id": "7D1v3SMxWlLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "prjcted = pca.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "9baqy7a7R7NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prjcted[idx_0,:].shape"
      ],
      "metadata": {
        "id": "R000UEIRaWV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(prjcted[idx_2,0] , prjcted[idx_2,1], c = 'cyan' , marker ='o' , label = le.inverse_transform([2]))#positive\n",
        "plt.scatter(prjcted[idx_0,0] , prjcted[idx_0,1], c = 'black' , marker ='^' , label = le.inverse_transform([0]))#neutral\n",
        "plt.scatter(prjcted[idx_1,0] , prjcted[idx_1,1], c = 'red' , marker ='+' , s= 200 , label = le.inverse_transform([1]))#negative\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pl3bfklfW4ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UMAP"
      ],
      "metadata": {
        "id": "tXWOtD2RVLje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "# Fit UMAP\n",
        "umap_proj = umap.UMAP(n_components=2, random_state=42).fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "kSVw6cn6VcFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(umap_proj[idx_2,0] , umap_proj[idx_2,1], c = 'cyan' , marker ='o' , s = 200 , label = le.inverse_transform([2]))#positive\n",
        "plt.scatter(umap_proj[idx_1,0] , umap_proj[idx_1,1], c = 'red' , marker ='+' , s= 200 , label = le.inverse_transform([1]))#negative\n",
        "plt.scatter(umap_proj[idx_0,0] , umap_proj[idx_0,1], c = 'black' , marker ='^' , label = le.inverse_transform([0]))#neutral\n",
        "plt.title('UMAP projection')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KB-JpoK6V7d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  # Save embeddings as a new column (as strings or use numpy for arrays)\n",
        "  df['embedding'] = embeddings\n",
        "  df.to_csv(\"with_embeddings.csv\", index=False)"
      ],
      "metadata": {
        "id": "nBt_LsoJRKvC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}